{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf01b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import copy\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import os, glob,shutil\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13783007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_for_action(action):\n",
    "    if action == 0:\n",
    "        return \"START\"\n",
    "    if action == 1:\n",
    "        return 'up-left'\n",
    "    elif action == 2:\n",
    "        return 'up-right'\n",
    "    elif action == 3:\n",
    "        return 'down-left'\n",
    "    elif action == 4:\n",
    "        return 'down-right'\n",
    "    elif action == 5:\n",
    "        return 'center'\n",
    "    elif action == 6:\n",
    "        return 'TRIGGER'\n",
    "\n",
    "\n",
    "def draw_sequences(i, k, step, action, draw, region_image, background, path_testing_folder, iou, reward,\n",
    "                   gt_mask, region_mask, image_name, save_boolean):\n",
    "    \n",
    "#     print(gt_mask.shape)\n",
    "#     print(region_mask.shape)\n",
    "    mask = Image.fromarray(255 * gt_mask)\n",
    "    mask_img = Image.fromarray(255 * region_mask)\n",
    "    image_offset = (1000 * step, 70)\n",
    "    text_offset = (1000 * step, 550)\n",
    "    masked_image_offset = (1000 * step, 1400)\n",
    "    mask_offset = (1000 * step, 700)\n",
    "    action_string = string_for_action(action)\n",
    "    myFont = ImageFont.truetype('../fonts/FreeMono.ttf', 30)\n",
    "    footnote = 'action: ' + action_string + ' ' + 'reward: ' + str(round(reward,2)) + ' Iou:' + str(round(iou,2))\n",
    "    draw.text(text_offset, str(footnote), (0, 0, 0),font=myFont)\n",
    "    region_image_np = np.einsum('ijk->jki',np.array(region_image.detach().cpu()))\n",
    "    \n",
    "    region_image_np = ((region_image_np - region_image_np.min())/(region_image_np.max() - region_image_np.min()))*255\n",
    "#     print(region_image_np.shape)\n",
    "    img_for_paste = Image.fromarray(region_image_np.astype(np.uint8))\n",
    "    background.paste(img_for_paste, image_offset)\n",
    "    background.paste(mask, mask_offset)\n",
    "    background.paste(mask_img, masked_image_offset)\n",
    "    file_name = path_testing_folder + '/' + image_name + str(i) + '_object_' + str(k) + '.png'\n",
    "    if save_boolean == 1:\n",
    "        background.save(file_name)\n",
    "    return background\n",
    "\n",
    "def draw_sequences_test(step, action, qval, draw, region_image, background, path_testing_folder,\n",
    "                        region_mask, image_name, save_boolean):\n",
    "    aux = np.asarray(region_image, np.uint8)\n",
    "    img_offset = (1000 * step, 70)\n",
    "    footnote_offset = (1000 * step, 550)\n",
    "    q_predictions_offset = (1000 * step, 500)\n",
    "    mask_img_offset = (1000 * step, 700)\n",
    "    img_for_paste = Image.fromarray(aux)\n",
    "    background.paste(img_for_paste, img_offset)\n",
    "    mask_img = Image.fromarray(255 * region_mask)\n",
    "    background.paste(mask_img, mask_img_offset)\n",
    "    footnote = 'action: ' + str(action)\n",
    "    q_val_predictions_text = str(qval)\n",
    "    draw.text(footnote_offset, footnote, (0, 0, 0))\n",
    "    draw.text(q_predictions_offset, q_val_predictions_text, (0, 0, 0))\n",
    "    file_name = path_testing_folder + image_name + '.png'\n",
    "    if save_boolean == 1:\n",
    "        background.save(file_name)\n",
    "    return background\n",
    "\n",
    "def mask_image_with_mean_background(mask_object_found, image):\n",
    "    new_image = image\n",
    "    size_image = np.shape(mask_object_found)\n",
    "    for j in range(size_image[0]):\n",
    "        for i in range(size_image[1]):\n",
    "            if mask_object_found[j][i] == 1:\n",
    "                    new_image[0, j, i] = 0.485\n",
    "                    new_image[1, j, i] = 0.456\n",
    "                    new_image[2, j, i] = 0.406\n",
    "#                     0.485, 0.456, 0.406\n",
    "    return new_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed077eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class res_model_no_top(nn.Module):\n",
    "    def __init__(self, output_layer):\n",
    "        super().__init__()\n",
    "        self.output_layer = output_layer\n",
    "        self.pretrained = models.resnet18(pretrained=True)\n",
    "        self.children_list = []\n",
    "        for n,c in self.pretrained.named_children():\n",
    "            self.children_list.append(c)\n",
    "            if n == self.output_layer:\n",
    "                break\n",
    "\n",
    "        self.net = nn.Sequential(*self.children_list)\n",
    "        self.pretrained = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "# Different actions that the agent can do\n",
    "number_of_actions = 6\n",
    "# Actions captures in the history vector\n",
    "actions_of_history = 4\n",
    "# Visual descriptor size\n",
    "feature_shape = 131072\n",
    "def get_state(image, history_vector):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    image_ = image.clone().detach().cpu().numpy()\n",
    "    image_ = np.resize(image_,(3,512,512))\n",
    "    \n",
    "    image_ = torch.from_numpy(image_)\n",
    "    image_ = image_.to(device)\n",
    "    with torch.no_grad():\n",
    "        get_features = res_model_no_top('layer4')\n",
    "        get_features = get_features.to(device)\n",
    "        descriptor_image = get_features(image_[None])\n",
    "#         print(descriptor_image.shape)\n",
    "        descriptor_image = descriptor_image.reshape((1,feature_shape))\n",
    "        history_vector = torch.reshape(history_vector, (1, number_of_actions*actions_of_history))\n",
    "        state = torch.hstack((descriptor_image, history_vector))\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b67182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_history_vector(history_vector, action):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    action_vector = np.zeros(number_of_actions)\n",
    "    history_vector = np.array(history_vector.clone().detach().cpu())\n",
    "    action_vector[action-1] = 1\n",
    "    size_history_vector = np.size(np.nonzero(history_vector))\n",
    "    updated_history_vector = np.zeros(number_of_actions*actions_of_history)\n",
    "    if size_history_vector < actions_of_history:\n",
    "        aux2 = 0\n",
    "        for l in range(number_of_actions*size_history_vector, number_of_actions*size_history_vector+number_of_actions - 1):\n",
    "            history_vector[l] = action_vector[aux2]\n",
    "            aux2 += 1\n",
    "        return torch.Tensor(history_vector).to(device)\n",
    "    else:\n",
    "        for j in range(0, number_of_actions*(actions_of_history-1) - 1):\n",
    "            updated_history_vector[j] = history_vector[j+number_of_actions]\n",
    "        aux = 0\n",
    "        for k in range(number_of_actions*(actions_of_history-1), number_of_actions*actions_of_history):\n",
    "            updated_history_vector[k] = action_vector[aux]\n",
    "            aux += 1\n",
    "        return torch.Tensor(updated_history_vector).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c3c03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward movement action\n",
    "reward_movement_action = 1\n",
    "# Reward terminal action\n",
    "reward_terminal_action = 3\n",
    "# IoU required to consider a positive detection\n",
    "iou_threshold = 0.5\n",
    "def get_reward_movement(iou, new_iou):\n",
    "    if new_iou > iou:\n",
    "        reward = reward_movement_action\n",
    "    else:\n",
    "        reward = - reward_movement_action\n",
    "    return reward\n",
    "\n",
    "\n",
    "def get_reward_trigger(new_iou):\n",
    "    if new_iou > iou_threshold:\n",
    "        reward = reward_terminal_action\n",
    "    else:\n",
    "        reward = - reward_terminal_action\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53a6bca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class q_network(nn.Module):\n",
    "    def __init__(self, num_hidden_layer, dim_hidden_layer, output_dim):\n",
    "        super(q_network, self).__init__()\n",
    "\n",
    "        \"\"\"CODE HERE: construct your Deep neural network\n",
    "        \"\"\"\n",
    "        self.input_linear = nn.Linear(131072+24,dim_hidden_layer)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linears = nn.ModuleList(nn.Linear(dim_hidden_layer,dim_hidden_layer) for i in range(num_hidden_layer))\n",
    "        self.relus = nn.ModuleList(nn.ReLU() for i in range(num_hidden_layer))\n",
    "        self.output_linear = nn.Linear(dim_hidden_layer, output_dim)\n",
    "    def forward(self, x):\n",
    "        \"\"\"CODE HERE: implement your forward propagation\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.input_linear(x)\n",
    "        x = self.relu1(x)\n",
    "        for linear,relu in zip(self.linears,self.relus):\n",
    "            x = linear(x)\n",
    "            x =  relu(x)\n",
    "        y = self.output_linear(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25389265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(img_mask, gt_mask):\n",
    "    gt_mask *= 1.0\n",
    "    img_and = np.multiply(img_mask, gt_mask)\n",
    "    img_or = img_mask + gt_mask\n",
    "    j = len(img_and[img_and>0])\n",
    "    i = len(img_and[img_or>0])\n",
    "    iou = float(float(j)/float(i))\n",
    "    return iou\n",
    "\n",
    "\n",
    "def calculate_overlapping(img_mask, gt_mask):\n",
    "    gt_mask *= 1.0\n",
    "    img_and = np.multiply(img_mask, gt_mask)\n",
    "    j = np.count_nonzero(img_and)\n",
    "    i = np.count_nonzero(gt_mask)\n",
    "    overlap = float(float(j)/float(i))\n",
    "    return overlap\n",
    "\n",
    "\n",
    "def follow_iou(gt_masks, mask, last_matrix, available_objects):\n",
    "    results = np.zeros(len(gt_masks))\n",
    "    for k in range(len(gt_masks)):\n",
    "        if available_objects[k] == 1:\n",
    "            gt_mask = gt_masks[k,:, :]\n",
    "            iou = calculate_iou(mask, gt_mask)\n",
    "            results[k] = iou\n",
    "        else:\n",
    "            results[k] = -1\n",
    "    max_result = max(results)\n",
    "    ind = np.argmax(results)\n",
    "    iou = last_matrix[ind]\n",
    "    new_iou = max_result\n",
    "    return iou, new_iou, results, ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3f11c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_names_in_data_set(data_set_name, path_voc):\n",
    "    file_path = path_voc + '/ImageSets/Main/' + data_set_name + '.txt'\n",
    "    f = open(file_path)\n",
    "    image_names = f.readlines()\n",
    "    image_names = [x.strip('\\n') for x in image_names]\n",
    "    if data_set_name.startswith(\"aeroplane\") | data_set_name.startswith(\"bird\") | data_set_name.startswith(\"cow\"):\n",
    "        return [x.split(None, 1)[0] for x in image_names]\n",
    "    else:\n",
    "        return [x.strip('\\n') for x in image_names]\n",
    "    \n",
    "def load_images_labels_in_data_set(data_set_name, path_voc):\n",
    "    file_path = path_voc + '/ImageSets/Main/' + data_set_name + '.txt'\n",
    "    f = open(file_path)\n",
    "    images_names = f.readlines()\n",
    "    images_names = [x.split(None, 1)[1] for x in images_names]\n",
    "    images_names = [x.strip('\\n') for x in images_names]\n",
    "    return images_names\n",
    "\n",
    "def get_all_images(image_names):\n",
    "    image_names_clean = copy.copy(image_names)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )])\n",
    "    images = []\n",
    "    for string in image_names:\n",
    "        try:\n",
    "            im = Image.open(string).convert('RGB')\n",
    "            im = preprocess(im)\n",
    "            images.append(im)\n",
    "        except:\n",
    "            image_names_clean.remove(image_name)\n",
    "            pass\n",
    "        \n",
    "    names = []\n",
    "    \n",
    "    for name in image_names_clean:\n",
    "        names.append(name.split('/')[-1].split('_')[0])\n",
    "    return images, names\n",
    "\n",
    "def get_bb_of_gt_from_pascal_xml_annotation(xml_name, voc_path):\n",
    "    string = voc_path + '/Annotations/' + xml_name + '.xml'\n",
    "    tree = ET.parse(string)\n",
    "    root = tree.getroot()\n",
    "    names = []\n",
    "    x_min = []\n",
    "    x_max = []\n",
    "    y_min = []\n",
    "    y_max = []\n",
    "    for child in root:\n",
    "        if child.tag == 'object':\n",
    "            for child2 in child:\n",
    "                if child2.tag == 'name':\n",
    "                    names.append(child2.text)\n",
    "                elif child2.tag == 'bndbox':\n",
    "                    for child3 in child2:\n",
    "                        if child3.tag == 'xmin':\n",
    "                            x_min.append(child3.text)\n",
    "                        elif child3.tag == 'xmax':\n",
    "                            x_max.append(child3.text)\n",
    "                        elif child3.tag == 'ymin':\n",
    "                            y_min.append(child3.text)\n",
    "                        elif child3.tag == 'ymax':\n",
    "                            y_max.append(child3.text)\n",
    "    category_and_bb = np.zeros([np.size(names), 5])\n",
    "    for i in range(np.size(names)):\n",
    "        category_and_bb[i][0] = get_id_of_class_name(names[i])\n",
    "        category_and_bb[i][1] = x_min[i]\n",
    "        category_and_bb[i][2] = x_max[i]\n",
    "        category_and_bb[i][3] = y_min[i]\n",
    "        category_and_bb[i][4] = y_max[i]\n",
    "    return category_and_bb\n",
    "def get_id_of_class_name (class_name):\n",
    "    if class_name == 'aeroplane':\n",
    "        return 1\n",
    "    elif class_name == 'bicycle':\n",
    "        return 2\n",
    "    elif class_name == 'bird':\n",
    "        return 3\n",
    "    elif class_name == 'boat':\n",
    "        return 4\n",
    "    elif class_name == 'bottle':\n",
    "        return 5\n",
    "    elif class_name == 'bus':\n",
    "        return 6\n",
    "    elif class_name == 'car':\n",
    "        return 7\n",
    "    elif class_name == 'cat':\n",
    "        return 8\n",
    "    elif class_name == 'chair':\n",
    "        return 9\n",
    "    elif class_name == 'cow':\n",
    "        return 10\n",
    "    elif class_name == 'diningtable':\n",
    "        return 11\n",
    "    elif class_name == 'dog':\n",
    "        return 12\n",
    "    elif class_name == 'horse':\n",
    "        return 13\n",
    "    elif class_name == 'motorbike':\n",
    "        return 14\n",
    "    elif class_name == 'person':\n",
    "        return 15\n",
    "    elif class_name == 'pottedplant':\n",
    "        return 16\n",
    "    elif class_name == 'sheep':\n",
    "        return 17\n",
    "    elif class_name == 'sofa':\n",
    "        return 18\n",
    "    elif class_name == 'train':\n",
    "        return 19\n",
    "    elif class_name == 'tvmonitor':\n",
    "        return 20\n",
    "    \n",
    "def generate_bounding_box_from_annotation(annotation, image_shape):\n",
    "    annotation = np.array(annotation,dtype=np.int16)\n",
    "    length_annotation = annotation.shape[0]\n",
    "    masks = np.zeros([length_annotation, image_shape[1], image_shape[2]])\n",
    "    for i in range(0, length_annotation):\n",
    "        masks[i, max(annotation[i][3],0):min(annotation[i][4],image_shape[1]), max(annotation[i][1],0):min(annotation[i][2],image_shape[1])] = 1\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3eda3b",
   "metadata": {},
   "source": [
    "## Vedai Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "591df404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotation_vedai(annot_path_vedai512, filename):\n",
    "    with open(annot_path_vedai512+f'/{filename}.txt') as fp:\n",
    "        annot = fp.readlines()\n",
    "        annotation = np.zeros((len(annot),5))\n",
    "        for i, line in enumerate(annot):\n",
    "            cords = line.split()\n",
    "            car_type = int(cords[3])\n",
    "            cords = [int(n) for n in cords[-8:]]\n",
    "            xmin = min(cords[-4:])\n",
    "            xmax = max(cords[-4:])\n",
    "\n",
    "            ymin = min(cords[:4])\n",
    "            ymax = max(cords[:4])\n",
    "\n",
    "            annotation[i] = np.array([car_type, ymin, ymax, xmin, xmax])\n",
    "    return annotation\n",
    "def show_image_mask(image, annotation):\n",
    "    image_ = image.copy()\n",
    "    mask = np.zeros(np.shape(image)[:2])\n",
    "    for annot in annotation:\n",
    "        image_[int(annot[3]):int(annot[4]),int(annot[1]):int(annot[2]),:] = 1\n",
    "        mask[int(annot[3]):int(annot[4]),int(annot[1]):int(annot[2])] = 1\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    ax1 = plt.subplot(1,3,1)\n",
    "    ax1.imshow(image)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Original')\n",
    "    ax2 = plt.subplot(1,3,2)\n",
    "    ax2.imshow(image_)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('Masked')\n",
    "    \n",
    "    ax3 = plt.subplot(1,3,3)\n",
    "    ax3.imshow(mask)\n",
    "    ax3.axis('off')\n",
    "    ax3.set_title('Mask')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08e127aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_voc07 = '../data/voc/VOC2007'\n",
    "path_model = '../model/'\n",
    "# image_names_ = np.array([load_images_names_in_data_set('trainval', path_voc07)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47f162af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_vedai512 = '../data/vedai/Vehicules512'\n",
    "annot_path_vedai512 = '../data/vedai/Annotations512'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "957fea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, image_names = get_all_images(image_names_[0], path_voc07)\n",
    "# len(images) == len(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a658da0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_512 = glob.glob(data_path_vedai512+'/*.png')\n",
    "images, image_names = get_all_images(image_paths_512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "349e261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## PARAMETERS ########\n",
    "\n",
    "# Class category of PASCAL that the RL agent will be searching\n",
    "class_object = 1\n",
    "# Scale of subregion for the hierarchical regions (to deal with 2/4, 3/4)\n",
    "scale_subregion = float(3)/4\n",
    "scale_mask = float(1)/(scale_subregion*4)\n",
    "# 1 if you want to obtain visualizations of the search for objects\n",
    "bool_draw = 1\n",
    "# How many steps can run the agent until finding one object\n",
    "number_of_steps = 14\n",
    "# Boolean to indicate if you want to use the two databases, or just one\n",
    "two_databases = 0\n",
    "epochs = 50\n",
    "gamma = 0.90\n",
    "epsilon = 1\n",
    "batch_size = 100\n",
    "# Pointer to where to store the last experience in the experience replay buffer,\n",
    "# actually there is a pointer for each PASCAL category, in case all categories\n",
    "# are trained at the same time\n",
    "h = np.zeros([1])\n",
    "# Each replay memory (one for each possible category) has a capacity of 100 experiences\n",
    "buffer_experience_replay = 1000\n",
    "# Init replay memories\n",
    "replay = [[] for i in range(1)]\n",
    "reward = 0\n",
    "\n",
    "path_testing_folder = '../results/vedai/train/.'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d76ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = q_network(2, 1024, 6)\n",
    "q_net = q_net.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=0.001)\n",
    "\n",
    "q_net_target = q_network(2, 1024, 6)\n",
    "q_net_target = q_net_target.to(device)\n",
    "q_net_target.load_state_dict(q_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b4eef236",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96d86b163e64605b675e07bd1c730b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2573 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/rohit/.conda/envs/pytorch/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/share/u/rohit/.conda/envs/pytorch/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fe6125538b4a8faf73bf6ba0399bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2573 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdbb8ddacd34cb892be7ac303333a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2573 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [51], line 117\u001b[0m\n\u001b[1;32m    115\u001b[0m     iou \u001b[38;5;241m=\u001b[39m new_iou\n\u001b[1;32m    116\u001b[0m history_vector \u001b[38;5;241m=\u001b[39m update_history_vector(history_vector, action)\n\u001b[0;32m--> 117\u001b[0m new_state \u001b[38;5;241m=\u001b[39m \u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregion_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Experience replay storage\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(replay[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m<\u001b[39m buffer_experience_replay:\n",
      "Cell \u001b[0;32mIn [23], line 33\u001b[0m, in \u001b[0;36mget_state\u001b[0;34m(image, history_vector)\u001b[0m\n\u001b[1;32m     31\u001b[0m image_ \u001b[38;5;241m=\u001b[39m image_\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 33\u001b[0m     get_features \u001b[38;5;241m=\u001b[39m \u001b[43mres_model_no_top\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlayer4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     get_features \u001b[38;5;241m=\u001b[39m get_features\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     35\u001b[0m     descriptor_image \u001b[38;5;241m=\u001b[39m get_features(image_[\u001b[38;5;28;01mNone\u001b[39;00m])\n",
      "Cell \u001b[0;32mIn [23], line 5\u001b[0m, in \u001b[0;36mres_model_no_top.__init__\u001b[0;34m(self, output_layer)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer \u001b[38;5;241m=\u001b[39m output_layer\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet18\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n,c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained\u001b[38;5;241m.\u001b[39mnamed_children():\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.9/site-packages/torchvision/models/_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs\u001b[38;5;241m.\u001b[39mkeys()), separate_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as positional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter(s) is deprecated since 0.13 and will be removed in 0.15. Please use keyword parameter(s) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.9/site-packages/torchvision/models/_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[1;32m    226\u001b[0m     kwargs[weights_param] \u001b[38;5;241m=\u001b[39m default_weights_arg\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.9/site-packages/torchvision/models/resnet.py:670\u001b[0m, in \u001b[0;36mresnet18\u001b[0;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;124;03m\"\"\"ResNet-18 from `Deep Residual Learning for Image Recognition <https://arxiv.org/pdf/1512.03385.pdf>`__.\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \n\u001b[1;32m    652\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;124;03m    :members:\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    668\u001b[0m weights \u001b[38;5;241m=\u001b[39m ResNet18_Weights\u001b[38;5;241m.\u001b[39mverify(weights)\n\u001b[0;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_resnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBasicBlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.9/site-packages/torchvision/models/resnet.py:298\u001b[0m, in \u001b[0;36m_resnet\u001b[0;34m(block, layers, weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m     _ovewrite_named_param(kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(weights\u001b[38;5;241m.\u001b[39mmeta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m--> 298\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mResNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(weights\u001b[38;5;241m.\u001b[39mget_state_dict(progress\u001b[38;5;241m=\u001b[39mprogress))\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.9/site-packages/torchvision/models/resnet.py:210\u001b[0m, in \u001b[0;36mResNet.__init__\u001b[0;34m(self, block, layers, num_classes, zero_init_residual, groups, width_per_group, replace_stride_with_dilation, norm_layer)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules():\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, nn\u001b[38;5;241m.\u001b[39mConv2d):\n\u001b[0;32m--> 210\u001b[0m         \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_normal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfan_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnonlinearity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, (nn\u001b[38;5;241m.\u001b[39mBatchNorm2d, nn\u001b[38;5;241m.\u001b[39mGroupNorm)):\n\u001b[1;32m    212\u001b[0m         nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mconstant_(m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/init.py:451\u001b[0m, in \u001b[0;36mkaiming_normal_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    449\u001b[0m std \u001b[38;5;241m=\u001b[39m gain \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(fan)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_curve = []\n",
    "epochs_id=0\n",
    "update_target = 100\n",
    "\n",
    "num_target = 0\n",
    "for j in range(epochs_id, epochs_id+epochs):\n",
    "    count_bool_draw = 0\n",
    "    for i in trange(len(images)):\n",
    "        count_bool_draw+=1\n",
    "        not_finished = 1\n",
    "        masked = 0\n",
    "        image = images[i]\n",
    "        image = image.to(device)\n",
    "        image_name = image_names[i]\n",
    "        annotation = get_annotation_vedai(annot_path_vedai512, image_name)\n",
    "        gt_masks = generate_bounding_box_from_annotation(annotation, image.shape)\n",
    "        region_mask = np.ones([image.shape[0], image.shape[1]])\n",
    "        shape_gt_masks = np.shape(gt_masks)\n",
    "#         available_objects = np.ones(np.size(array_classes_gt_objects))\n",
    "         # number of masks\n",
    "        for k in range(shape_gt_masks[0]):\n",
    "            background = Image.new('RGBA', (14000, 2500), (255, 255, 255, 255))\n",
    "            draw = ImageDraw.Draw(background)\n",
    "            gt_mask = gt_masks[k]\n",
    "            step = 0\n",
    "            new_iou = 0\n",
    "            if count_bool_draw%20 == 0:\n",
    "                bool_draw = 1\n",
    "            else:\n",
    "                bool_draw = 0\n",
    "            region_image = image.clone().detach()\n",
    "            offset = (0, 0)\n",
    "            size_mask = (image.shape[1], image.shape[2])\n",
    "            original_shape = size_mask\n",
    "            region_mask = np.ones([image.shape[1], image.shape[2]])\n",
    "            old_region_mask = np.zeros([image.shape[1], image.shape[2]])\n",
    "            available_objects = np.ones(gt_masks.shape[0])\n",
    "            last_matrix = np.zeros(gt_masks.shape[0])\n",
    "            if masked == 1:\n",
    "                for p in range(gt_masks.shape[0]):\n",
    "                    overlap = calculate_overlapping(old_region_mask, gt_masks[p,:,:])\n",
    "                    if overlap > 0.60:\n",
    "                        available_objects[p] = 0\n",
    "            # We check if there are still obejcts to be found\n",
    "            if np.count_nonzero(available_objects) == 0:\n",
    "                not_finished = 0\n",
    "\n",
    "            iou, new_iou, last_matrix, index = follow_iou(gt_masks, region_mask, last_matrix, available_objects)\n",
    "            new_iou = iou\n",
    "            gt_mask = gt_masks[index,:, :]\n",
    "            history_vector = torch.zeros([24])\n",
    "            history_vector = history_vector.to(device)\n",
    "            # computation of the initial state\n",
    "            state = get_state(region_image, history_vector)\n",
    "            # status indicates whether the agent is still alive and has not triggered the terminal action\n",
    "            status = 1\n",
    "            action = 0\n",
    "            reward = 0\n",
    "            if step > number_of_steps:\n",
    "                background = draw_sequences(j, k, step, action, draw, region_image, background,\n",
    "                                            path_testing_folder, iou, reward, gt_mask, region_mask, image_name,\n",
    "                                            bool_draw)\n",
    "                step += 1\n",
    "\n",
    "            while (status == 1) & (step < number_of_steps) & not_finished:\n",
    "                qval = q_net(state)\n",
    "                background = draw_sequences(j, k, step, action, draw, region_image, background,\n",
    "                                    path_testing_folder, iou, reward, gt_mask, region_mask, image_name,\n",
    "                                    bool_draw)\n",
    "                step += 1\n",
    "                # we force terminal action in case actual IoU is higher than 0.5, to train faster the agent\n",
    "                if (i < 100) & (new_iou > 0.5):\n",
    "                    action = 6\n",
    "                # epsilon-greedy policy\n",
    "                elif random.random() < epsilon:\n",
    "                    action = np.random.randint(1, 7)\n",
    "                else:\n",
    "                    action = int(torch.argmax(qval))+1\n",
    "                # terminal action\n",
    "                if action == 6:\n",
    "                    iou, new_iou, last_matrix, index = follow_iou(gt_masks, region_mask, last_matrix, available_objects)\n",
    "                    gt_mask = gt_masks[index, :, :]\n",
    "                    reward = get_reward_trigger(new_iou)\n",
    "                    background = draw_sequences(j, k, step, action, draw, region_image, background,\n",
    "                                                path_testing_folder, iou, reward, gt_mask, region_mask, image_name,\n",
    "                                                bool_draw)\n",
    "                    step += 1\n",
    "                else:\n",
    "                    region_mask = np.zeros(original_shape)\n",
    "                    size_mask = (size_mask[0] * scale_subregion, size_mask[1] * scale_subregion)\n",
    "                    if action == 1:\n",
    "                        offset_aux = (0, 0)\n",
    "                    elif action == 2:\n",
    "                        offset_aux = (0, size_mask[1] * scale_mask)\n",
    "                        offset = (offset[0], offset[1] + size_mask[1] * scale_mask)\n",
    "                    elif action == 3:\n",
    "                        offset_aux = (size_mask[0] * scale_mask, 0)\n",
    "                        offset = (offset[0] + size_mask[0] * scale_mask, offset[1])\n",
    "                    elif action == 4:\n",
    "                        offset_aux = (size_mask[0] * scale_mask, \n",
    "                                      size_mask[1] * scale_mask)\n",
    "                        offset = (offset[0] + size_mask[0] * scale_mask,\n",
    "                                  offset[1] + size_mask[1] * scale_mask)\n",
    "                    elif action == 5:\n",
    "                        offset_aux = (size_mask[0] * scale_mask / 2,\n",
    "                                      size_mask[0] * scale_mask / 2)\n",
    "                        offset = (offset[0] + size_mask[0] * scale_mask / 2,\n",
    "                                  offset[1] + size_mask[0] * scale_mask / 2)\n",
    "                    region_image = region_image[:,int(offset_aux[0]):int(offset_aux[0] + size_mask[0]),\n",
    "                                   int(offset_aux[1]):int(offset_aux[1] + size_mask[1])]\n",
    "                    region_mask[int(offset[0]):int(offset[0] + size_mask[0]), int(offset[1]):int(offset[1] + size_mask[1])] = 1\n",
    "                    iou, new_iou, last_matrix, index = follow_iou(gt_masks, region_mask, last_matrix, available_objects)\n",
    "                    gt_mask = gt_masks[index, :, :]\n",
    "                    reward = get_reward_movement(iou, new_iou)\n",
    "                    iou = new_iou\n",
    "                history_vector = update_history_vector(history_vector, action)\n",
    "                new_state = get_state(region_image, history_vector)\n",
    "                # Experience replay storage\n",
    "                if len(replay[0]) < buffer_experience_replay:\n",
    "                    replay[0].append((state, action, reward, new_state))\n",
    "                else:\n",
    "#                     print('Training')\n",
    "                    if h[0] < (buffer_experience_replay-1):\n",
    "                        h[0] += 1\n",
    "                    else:\n",
    "                        h[0] = 0\n",
    "                    h_aux = h[0]\n",
    "                    h_aux = int(h_aux)\n",
    "#                     print(replay[0][h_aux])\n",
    "                    replay[0][h_aux] = (state, action, reward, new_state)\n",
    "#                     print(replay[0][h_aux])\n",
    "                    minibatch = random.sample(replay[0], batch_size)\n",
    "                    X_train = []\n",
    "                    y_train = []\n",
    "                    # we pick from the replay memory a sampled minibatch and generate the training samples\n",
    "                    for memory in minibatch:\n",
    "                        old_state, action, reward, new_state = memory\n",
    "#                         print(action)\n",
    "                        with torch.no_grad():\n",
    "                            old_qval = q_net_target(old_state)\n",
    "                            newQ = q_net(new_state)\n",
    "                            newQ_target = q_net_target(new_state)\n",
    "                        maxQ_arg = int(newQ.argmax())\n",
    "                        maxQ = float(newQ_target[0,maxQ_arg])\n",
    "                        y = torch.zeros((1, 6))\n",
    "                        y = old_qval\n",
    "                        y = y\n",
    "                        if action != 6: #non-terminal state\n",
    "                            update = (reward + (gamma * maxQ))\n",
    "                        else: #terminal state\n",
    "                            update = reward\n",
    "                        y[0][action-1] = update #target output\n",
    "                        X_train.append(old_state)\n",
    "                        y_train.append(y)\n",
    "                    X_train = torch.cat(X_train)\n",
    "                    y_train = torch.cat(y_train)\n",
    "#                     print(X_train.shape)\n",
    "#                     print(y_train.shape)\n",
    "                    optimizer.zero_grad()\n",
    "                    y_pred = q_net(X_train)\n",
    "#                     hist = q_net.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1, verbose=0)\n",
    "                    loss = criterion(y_pred, y_train)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    loss_curve.append(loss.item())\n",
    "#                     print(q_net.linears[0].weight[0][:10])\n",
    "                    state = new_state\n",
    "                    num_target+=1\n",
    "                    if num_target%update_target == 0:\n",
    "                        q_net_target.load_state_dict(q_net.state_dict())\n",
    "                if action == 6:\n",
    "                    status = 0\n",
    "                    masked = 1\n",
    "                    # we mask object found with ground-truth so that agent learns faster\n",
    "                    image = mask_image_with_mean_background(gt_mask, image)\n",
    "                else:\n",
    "                    masked = 0\n",
    "    if epsilon > 0.1:\n",
    "        epsilon -= 0.1\n",
    "    string = path_model + '/model_epoch_' + str(i) + '.h5'\n",
    "    string2 = path_model + '/model.h5'\n",
    "    torch.save(q_net.state_dict(), string)\n",
    "    torch.save(q_net.state_dict(), string2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52989e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f8e2ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = path_model + '/model_epoch_' + str(j) + '.h5'\n",
    "string2 = path_model + '/model.h5'\n",
    "torch.save(q_net.state_dict(), string)\n",
    "torch.save(q_net.state_dict(), string2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cec59183",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mepoch\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch' is not defined"
     ]
    }
   ],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ca41c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
