{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2529e6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2, numpy as np\n",
    "import time\n",
    "import math as mth\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import scipy.io\n",
    "from keras.models import Sequential\n",
    "from keras import initializations\n",
    "from keras.initializations import normal, identity\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras import backend as K\n",
    "import random\n",
    "from scipy import ndimage\n",
    "from keras.preprocessing import image as keras_image_helper\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import math\n",
    "import numpy, scipy\n",
    "from scipy import interpolate\n",
    "import scipy.ndimage\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa27183",
   "metadata": {},
   "source": [
    "## Helper functions to manipulate image and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df31ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the feature size is of 7x7xp, being p the number of channels\n",
    "feature_size = 7\n",
    "# the relative scale reduction of the shallower feature map compared to the initial image input\n",
    "scale_reduction_shallower_feature = 16\n",
    "# the relative scale reduction of the deeper feature map compared to the initial image input\n",
    "scale_reduction_deeper_feature = 32\n",
    "# scaling of the input image\n",
    "factor_x_input = float(1)\n",
    "factor_y_input = float(1)\n",
    "\n",
    "\n",
    "# Interpolation of 2d features for a single channel of a feature map\n",
    "def interpolate_2d_features(features):\n",
    "    out_size = feature_size\n",
    "    x = np.arange(features.shape[0])\n",
    "    y = np.arange(features.shape[1])\n",
    "    z = features\n",
    "    xx = np.linspace(x.min(), x.max(), out_size)\n",
    "    yy = np.linspace(y.min(), y.max(), out_size)\n",
    "    new_kernel = interpolate.RectBivariateSpline(x, y, z, kx=1, ky=1)\n",
    "    kernel_out = new_kernel(xx, yy)\n",
    "    return kernel_out\n",
    "\n",
    "\n",
    "# Interpolation 2d of each channel, so we obtain 3d interpolated feature maps\n",
    "def interpolate_3d_features(features):\n",
    "    new_features = np.zeros([512, feature_size, feature_size])\n",
    "    for i in range(features.shape[0]):\n",
    "        new_features[i, :, :] = interpolate_2d_features(features[i, :, :])\n",
    "    return new_features\n",
    "\n",
    "\n",
    "def pop_layer(model):\n",
    "    if not model.outputs:\n",
    "        raise Exception('Sequential model cannot be popped: model is empty.')\n",
    "    model.layers.pop()\n",
    "    if not model.layers:\n",
    "        model.outputs = []\n",
    "        model.inbound_nodes = []\n",
    "        model.outbound_nodes = []\n",
    "    else:\n",
    "        model.layers[-1].outbound_nodes = []\n",
    "        model.outputs = [model.layers[-1].output]\n",
    "    model.built = False\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_convolutional_vgg16_compiled(vgg_weights_path):\n",
    "    model_vgg = obtain_compiled_vgg_16(vgg_weights_path)\n",
    "    for i in range(0, 6):\n",
    "        model_vgg = pop_layer(model_vgg)\n",
    "    return model_vgg\n",
    "\n",
    "\n",
    "def get_feature_maps(model, img):\n",
    "    return [get_feature_map_4(model, img), get_feature_map_8(model, img)]\n",
    "\n",
    "\n",
    "# get deeper feature map\n",
    "def get_feature_map_8(model, im):\n",
    "    im = im.astype(np.float32)\n",
    "    dim_ordering = K.image_dim_ordering()\n",
    "    if dim_ordering == 'th':\n",
    "        # 'RGB'->'BGR'\n",
    "        im = im[::-1, :, :]\n",
    "        # Zero-center by mean pixel\n",
    "        im[0, :, :] -= 103.939\n",
    "        im[1, :, :] -= 116.779\n",
    "        im[2, :, :] -= 123.68\n",
    "    else:\n",
    "        # 'RGB'->'BGR'\n",
    "        im = im[:, :, ::-1]\n",
    "        # Zero-center by mean pixel\n",
    "        im[:, :, 0] -= 103.939\n",
    "        im[:, :, 1] -= 116.779\n",
    "        im[:, :, 2] -= 123.68\n",
    "    im = im.transpose((2, 0, 1))\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    inputs = [K.learning_phase()] + model.inputs\n",
    "    _convout1_f = K.function(inputs, model.outputs)\n",
    "    feature_map = _convout1_f([0] + [im])\n",
    "    feature_map = np.array([feature_map])\n",
    "    feature_map = feature_map[0, 0, 0, :, :, :]\n",
    "    return feature_map\n",
    "\n",
    "\n",
    "# get shallower feature map\n",
    "def get_feature_map_4(model, im):\n",
    "    im = im.astype(np.float32)\n",
    "    dim_ordering = K.image_dim_ordering()\n",
    "    if dim_ordering == 'th':\n",
    "        # 'RGB'->'BGR'\n",
    "        im = im[::-1, :, :]\n",
    "        # Zero-center by mean pixel\n",
    "        im[0, :, :] -= 103.939\n",
    "        im[1, :, :] -= 116.779\n",
    "        im[2, :, :] -= 123.68\n",
    "    else:\n",
    "        # 'RGB'->'BGR'\n",
    "        im = im[:, :, ::-1]\n",
    "        # Zero-center by mean pixel\n",
    "        im[:, :, 0] -= 103.939\n",
    "        im[:, :, 1] -= 116.779\n",
    "        im[:, :, 2] -= 123.68\n",
    "    im = im.transpose((2, 0, 1))\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    inputs = [K.learning_phase()] + model.inputs\n",
    "    _convout1_f = K.function(inputs, [model.layers[23].output])\n",
    "    feature_map = _convout1_f([0] + [im])\n",
    "    feature_map = np.array([feature_map])\n",
    "    feature_map = feature_map[0, 0, 0, :, :, :]\n",
    "    return feature_map\n",
    "\n",
    "\n",
    "def crop_roi(feature_map, coordinates):\n",
    "    return feature_map[:, coordinates[0]:coordinates[0]+coordinates[2], coordinates[1]:coordinates[1]+coordinates[3]]\n",
    "\n",
    "\n",
    "# this method decides whether to use the deeper or the shallower feature map\n",
    "# and then crops and interpolates if necessary the features to obtain a final descriptor of 7x7xp\n",
    "def obtain_descriptor_from_feature_map(feature_maps, region_coordinates):\n",
    "    initial_width = region_coordinates[2]*factor_x_input\n",
    "    initial_height = region_coordinates[3]*factor_y_input\n",
    "    scale_aux = math.sqrt(initial_height*initial_width)/math.sqrt(feature_size*feature_size)\n",
    "    if scale_aux > scale_reduction_deeper_feature:\n",
    "        scale = scale_reduction_deeper_feature\n",
    "        feature_map = feature_maps[1]\n",
    "    else:\n",
    "        scale = scale_reduction_shallower_feature\n",
    "        feature_map = feature_maps[0]\n",
    "    new_width = initial_width/scale\n",
    "    new_height = initial_height/scale\n",
    "    if new_width < feature_size:\n",
    "        new_width = feature_size\n",
    "    if new_height < feature_size:\n",
    "        new_height = feature_size\n",
    "    xo = region_coordinates[0]/scale\n",
    "    yo = region_coordinates[1]/scale\n",
    "    feat = np.array([feature_map])\n",
    "    if new_width + xo > feat.shape[2]:\n",
    "        xo = feat.shape[2] - new_width\n",
    "    if new_height + yo > feat.shape[3]:\n",
    "        yo = feat.shape[3] - new_height\n",
    "    if xo < 0:\n",
    "        xo = 0\n",
    "    if yo < 0:\n",
    "        yo = 0\n",
    "    new_coordinates = np.array([xo, yo, new_width, new_height])\n",
    "    roi = crop_roi(feature_map, new_coordinates)\n",
    "    if roi.shape[1] < feature_size & roi.shape[2] < feature_size:\n",
    "        features = interpolate_3d_features(roi)\n",
    "    elif roi.shape[2] < feature_size:\n",
    "        features = interpolate_3d_features(roi)\n",
    "    elif roi.shape[1] < feature_size:\n",
    "        features = interpolate_3d_features(roi)\n",
    "    else:\n",
    "        features = extract_features_from_roi(roi)\n",
    "    return features\n",
    "\n",
    "\n",
    "# ROI-pooling features\n",
    "def extract_features_from_roi(roi):\n",
    "    roi_width = roi.shape[1]\n",
    "    roi_height = roi.shape[2]\n",
    "    new_width = roi_width / feature_size\n",
    "    new_height = roi_height / feature_size\n",
    "    pooled_values = np.zeros([feature_size, feature_size, 512])\n",
    "    for j in range(512):\n",
    "        for i in range(feature_size):\n",
    "            for k in range(feature_size):\n",
    "                if k == (feature_size-1) & i == (feature_size-1):\n",
    "                    patch = roi[j, i * new_width:roi_width, k * new_height:roi_height]\n",
    "                elif k == (feature_size-1):\n",
    "                    patch = roi[j, i * new_width:(i + 1) * new_width, k * new_height:roi_height]\n",
    "                elif i == (feature_size-1):\n",
    "                    patch = roi[j, i * new_width:roi_width, k * new_height:(k + 1) * new_height]\n",
    "                else:\n",
    "                    patch = roi[j, i * new_width:(i + 1) * new_width, k * new_height:(k + 1) * new_height]\n",
    "                pooled_values[i, k, j] = np.max(patch)\n",
    "    return pooled_values\n",
    "\n",
    "\n",
    "def calculate_all_initial_feature_maps(images, model, image_names):\n",
    "    initial_feature_maps = []\n",
    "    for z in range(np.size(image_names)):\n",
    "        initial_feature_maps.append(get_feature_maps(model, np.array(images[z])))\n",
    "    return initial_feature_maps\n",
    "\n",
    "\n",
    "def get_image_descriptor_for_image(image, model):\n",
    "    im = cv2.resize(image, (224, 224)).astype(np.float32)\n",
    "    dim_ordering = K.image_dim_ordering()\n",
    "    if dim_ordering == 'th':\n",
    "        # 'RGB'->'BGR'\n",
    "        im = im[::-1, :, :]\n",
    "        # Zero-center by mean pixel\n",
    "        im[0, :, :] -= 103.939\n",
    "        im[1, :, :] -= 116.779\n",
    "        im[2, :, :] -= 123.68\n",
    "    else:\n",
    "        # 'RGB'->'BGR'\n",
    "        im = im[:, :, ::-1]\n",
    "        # Zero-center by mean pixel\n",
    "        im[:, :, 0] -= 103.939\n",
    "        im[:, :, 1] -= 116.779\n",
    "        im[:, :, 2] -= 123.68\n",
    "    im = im.transpose((2, 0, 1))\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    inputs = [K.learning_phase()] + model.inputs\n",
    "    _convout1_f = K.function(inputs, [model.layers[33].output])\n",
    "    return _convout1_f([0] + [im])\n",
    "\n",
    "\n",
    "def get_conv_image_descriptor_for_image(image, model):\n",
    "    im = cv2.resize(image, (224, 224)).astype(np.float32)\n",
    "    dim_ordering = K.image_dim_ordering()\n",
    "    if dim_ordering == 'th':\n",
    "        # 'RGB'->'BGR'\n",
    "        im = im[::-1, :, :]\n",
    "        # Zero-center by mean pixel\n",
    "        im[0, :, :] -= 103.939\n",
    "        im[1, :, :] -= 116.779\n",
    "        im[2, :, :] -= 123.68\n",
    "    else:\n",
    "        # 'RGB'->'BGR'\n",
    "        im = im[:, :, ::-1]\n",
    "        # Zero-center by mean pixel\n",
    "        im[:, :, 0] -= 103.939\n",
    "        im[:, :, 1] -= 116.779\n",
    "        im[:, :, 2] -= 123.68\n",
    "    im = im.transpose((2, 0, 1))\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    inputs = [K.learning_phase()] + model.inputs\n",
    "    _convout1_f = K.function(inputs, [model.layers[31].output])\n",
    "    return _convout1_f([0] + [im])\n",
    "\n",
    "\n",
    "def obtain_compiled_vgg_16(vgg_weights_path):\n",
    "    model = vgg_16(vgg_weights_path)\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg_16(weights_path=None):\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(3, 224, 224)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e40dc",
   "metadata": {},
   "source": [
    "## Helper Functions to deal with reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ddd6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different actions that the agent can do\n",
    "number_of_actions = 6\n",
    "# Actions captures in the history vector\n",
    "actions_of_history = 4\n",
    "# Visual descriptor size\n",
    "visual_descriptor_size = 25088\n",
    "# Reward movement action\n",
    "reward_movement_action = 1\n",
    "# Reward terminal action\n",
    "reward_terminal_action = 3\n",
    "# IoU required to consider a positive detection\n",
    "iou_threshold = 0.5\n",
    "\n",
    "\n",
    "def update_history_vector(history_vector, action):\n",
    "    action_vector = np.zeros(number_of_actions)\n",
    "    action_vector[action-1] = 1\n",
    "    size_history_vector = np.size(np.nonzero(history_vector))\n",
    "    updated_history_vector = np.zeros(number_of_actions*actions_of_history)\n",
    "    if size_history_vector < actions_of_history:\n",
    "        aux2 = 0\n",
    "        for l in range(number_of_actions*size_history_vector, number_of_actions*size_history_vector+number_of_actions - 1):\n",
    "            history_vector[l] = action_vector[aux2]\n",
    "            aux2 += 1\n",
    "        return history_vector\n",
    "    else:\n",
    "        for j in range(0, number_of_actions*(actions_of_history-1) - 1):\n",
    "            updated_history_vector[j] = history_vector[j+number_of_actions]\n",
    "        aux = 0\n",
    "        for k in range(number_of_actions*(actions_of_history-1), number_of_actions*actions_of_history):\n",
    "            updated_history_vector[k] = action_vector[aux]\n",
    "            aux += 1\n",
    "        return updated_history_vector\n",
    "\n",
    "\n",
    "def get_state(image, history_vector, model_vgg):\n",
    "    descriptor_image = get_conv_image_descriptor_for_image(image, model_vgg)\n",
    "    descriptor_image = np.reshape(descriptor_image, (visual_descriptor_size, 1))\n",
    "    history_vector = np.reshape(history_vector, (number_of_actions*actions_of_history, 1))\n",
    "    state = np.vstack((descriptor_image, history_vector))\n",
    "    return state\n",
    "\n",
    "\n",
    "def get_state_pool45(history_vector,  region_descriptor):\n",
    "    history_vector = np.reshape(history_vector, (24, 1))\n",
    "    return np.vstack((region_descriptor, history_vector))\n",
    "\n",
    "\n",
    "def get_reward_movement(iou, new_iou):\n",
    "    if new_iou > iou:\n",
    "        reward = reward_movement_action\n",
    "    else:\n",
    "        reward = - reward_movement_action\n",
    "    return reward\n",
    "\n",
    "\n",
    "def get_reward_trigger(new_iou):\n",
    "    if new_iou > iou_threshold:\n",
    "        reward = reward_terminal_action\n",
    "    else:\n",
    "        reward = - reward_terminal_action\n",
    "    return reward\n",
    "\n",
    "\n",
    "def get_q_network(weights_path):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, init=lambda shape, name: normal(shape, scale=0.01, name=name), input_shape=(25112,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1024, init=lambda shape, name: normal(shape, scale=0.01, name=name)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(6, init=lambda shape, name: normal(shape, scale=0.01, name=name)))\n",
    "    model.add(Activation('linear'))\n",
    "    adam = Adam(lr=1e-6)\n",
    "    model.compile(loss='mse', optimizer=adam)\n",
    "    if weights_path != \"0\":\n",
    "        model.load_weights(weights_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_array_of_q_networks_for_pascal(weights_path, class_object):\n",
    "    q_networks = []\n",
    "    if weights_path == \"0\":\n",
    "        for i in range(20):\n",
    "            q_networks.append(get_q_network(\"0\"))\n",
    "    else:\n",
    "        for i in range(20):\n",
    "            if i == (class_object-1):\n",
    "                q_networks.append(get_q_network(weights_path + \"/model\" + str(i) + \"h5\"))\n",
    "            else:\n",
    "                q_networks.append(get_q_network(\"0\"))\n",
    "    return np.array([q_networks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264db2df",
   "metadata": {},
   "source": [
    "## Helper functions to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "557bd4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_font = \"../fonts/FreeMono.ttf\"\n",
    "font = ImageFont.truetype(path_font, 32)\n",
    "\n",
    "\n",
    "def string_for_action(action):\n",
    "    if action == 0:\n",
    "        return \"START\"\n",
    "    if action == 1:\n",
    "        return 'up-left'\n",
    "    elif action == 2:\n",
    "        return 'up-right'\n",
    "    elif action == 3:\n",
    "        return 'down-left'\n",
    "    elif action == 4:\n",
    "        return 'down-right'\n",
    "    elif action == 5:\n",
    "        return 'center'\n",
    "    elif action == 6:\n",
    "        return 'TRIGGER'\n",
    "\n",
    "\n",
    "def draw_sequences(i, k, step, action, draw, region_image, background, path_testing_folder, iou, reward,\n",
    "                   gt_mask, region_mask, image_name, save_boolean):\n",
    "    mask = Image.fromarray(255 * gt_mask)\n",
    "    mask_img = Image.fromarray(255 * region_mask)\n",
    "    image_offset = (1000 * step, 70)\n",
    "    text_offset = (1000 * step, 550)\n",
    "    masked_image_offset = (1000 * step, 1400)\n",
    "    mask_offset = (1000 * step, 700)\n",
    "    action_string = string_for_action(action)\n",
    "    footnote = 'action: ' + action_string + ' ' + 'reward: ' + str(reward) + ' Iou:' + str(iou)\n",
    "    draw.text(text_offset, str(footnote), (0, 0, 0), font=font)\n",
    "    img_for_paste = Image.fromarray(region_image)\n",
    "    background.paste(img_for_paste, image_offset)\n",
    "    background.paste(mask, mask_offset)\n",
    "    background.paste(mask_img, masked_image_offset)\n",
    "    file_name = path_testing_folder + '/' + image_name + str(i) + '_object_' + str(k) + '.png'\n",
    "    if save_boolean == 1:\n",
    "        background.save(file_name)\n",
    "    return background\n",
    "\n",
    "\n",
    "def draw_sequences_test(step, action, qval, draw, region_image, background, path_testing_folder,\n",
    "                        region_mask, image_name, save_boolean):\n",
    "    aux = np.asarray(region_image, np.uint8)\n",
    "    img_offset = (1000 * step, 70)\n",
    "    footnote_offset = (1000 * step, 550)\n",
    "    q_predictions_offset = (1000 * step, 500)\n",
    "    mask_img_offset = (1000 * step, 700)\n",
    "    img_for_paste = Image.fromarray(aux)\n",
    "    background.paste(img_for_paste, img_offset)\n",
    "    mask_img = Image.fromarray(255 * region_mask)\n",
    "    background.paste(mask_img, mask_img_offset)\n",
    "    footnote = 'action: ' + str(action)\n",
    "    qval_new = qval.copy()\n",
    "    qval_new = [round(s,2) for s in qval_new[0]]\n",
    "    q_val_predictions_text = str(qval_new)\n",
    "    draw.text(footnote_offset, footnote, (0, 0, 0), font=font)\n",
    "    draw.text(q_predictions_offset, q_val_predictions_text, (0, 0, 0), font=font)\n",
    "    file_name = path_testing_folder + image_name + '.png'\n",
    "    if save_boolean == 1:\n",
    "        background.save(file_name)\n",
    "    return background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca0e03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_iou(img_mask, gt_mask):\n",
    "    gt_mask *= 1.0\n",
    "    img_and = cv2.bitwise_and(img_mask, gt_mask)\n",
    "    img_or = cv2.bitwise_or(img_mask, gt_mask)\n",
    "    j = np.count_nonzero(img_and)\n",
    "    i = np.count_nonzero(img_or)\n",
    "    iou = float(float(j)/float(i))\n",
    "    return iou\n",
    "\n",
    "\n",
    "def calculate_overlapping(img_mask, gt_mask):\n",
    "    gt_mask *= 1.0\n",
    "    img_and = cv2.bitwise_and(img_mask, gt_mask)\n",
    "    j = np.count_nonzero(img_and)\n",
    "    i = np.count_nonzero(gt_mask)\n",
    "    overlap = float(float(j)/float(i))\n",
    "    return overlap\n",
    "\n",
    "\n",
    "def follow_iou(gt_masks, mask, array_classes_gt_objects, object_id, last_matrix, available_objects):\n",
    "    results = np.zeros([np.size(array_classes_gt_objects), 1])\n",
    "    for k in range(np.size(array_classes_gt_objects)):\n",
    "        if array_classes_gt_objects[k] == object_id:\n",
    "            if available_objects[k] == 1:\n",
    "                gt_mask = gt_masks[:, :, k]\n",
    "                iou = calculate_iou(mask, gt_mask)\n",
    "                results[k] = iou\n",
    "            else:\n",
    "                results[k] = -1\n",
    "    max_result = max(results)\n",
    "    ind = np.argmax(results)\n",
    "    iou = last_matrix[ind]\n",
    "    new_iou = max_result\n",
    "    return iou, new_iou, results, ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b393d5f",
   "metadata": {},
   "source": [
    "## Random Image Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "966d5caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_ids(annotations):\n",
    "    all_ids = []\n",
    "    for i in range(len(annotations)):\n",
    "        all_ids.append(get_ids_objects_from_annotation(annotations[i]))\n",
    "    return all_ids\n",
    "\n",
    "\n",
    "def get_all_images(image_names, path_voc):\n",
    "    images = []\n",
    "    for j in range(np.size(image_names)):\n",
    "        image_name = image_names[0][j]\n",
    "        string = path_voc + '/JPEGImages/' + image_name + '.jpg'\n",
    "        images.append(keras_image_helper.load_img(string, False))\n",
    "    return images\n",
    "\n",
    "\n",
    "def get_all_images_pool(image_names, path_voc):\n",
    "    images = []\n",
    "    for j in range(np.size(image_names)):\n",
    "        image_name = image_names[j]\n",
    "        string = path_voc + '/JPEGImages/' + image_name + '.jpg'\n",
    "        images.append(keras_image_helper.load_img(string, False))\n",
    "    return images\n",
    "\n",
    "\n",
    "def load_images_names_in_data_set(data_set_name, path_voc):\n",
    "    file_path = path_voc + '/ImageSets/Main/' + data_set_name + '.txt'\n",
    "    f = open(file_path)\n",
    "    image_names = f.readlines()\n",
    "    image_names = [x.strip('\\n') for x in image_names]\n",
    "    if data_set_name.startswith(\"aeroplane\") | data_set_name.startswith(\"bird\") | data_set_name.startswith(\"cow\"):\n",
    "        return [x.split(None, 1)[0] for x in image_names]\n",
    "    else:\n",
    "        return [x.strip('\\n') for x in image_names]\n",
    "\n",
    "\n",
    "def load_images_labels_in_data_set(data_set_name, path_voc):\n",
    "    file_path = path_voc + '/ImageSets/Main/' + data_set_name + '.txt'\n",
    "    f = open(file_path)\n",
    "    images_names = f.readlines()\n",
    "    images_names = [x.split(None, 1)[1] for x in images_names]\n",
    "    images_names = [x.strip('\\n') for x in images_names]\n",
    "    return images_names\n",
    "\n",
    "\n",
    "def mask_image_with_mean_background(mask_object_found, image):\n",
    "    new_image = image\n",
    "    size_image = np.shape(mask_object_found)\n",
    "    for j in range(size_image[0]):\n",
    "        for i in range(size_image[1]):\n",
    "            if mask_object_found[j][i] == 1:\n",
    "                    new_image[j, i, 0] = 103.939\n",
    "                    new_image[j, i, 1] = 116.779\n",
    "                    new_image[j, i, 2] = 123.68\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e5e753",
   "metadata": {},
   "source": [
    "## VOC toolkit helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2ebb1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bb_of_gt_from_pascal_xml_annotation(xml_name, voc_path):\n",
    "    string = voc_path + '/Annotations/' + xml_name + '.xml'\n",
    "    tree = ET.parse(string)\n",
    "    root = tree.getroot()\n",
    "    names = []\n",
    "    x_min = []\n",
    "    x_max = []\n",
    "    y_min = []\n",
    "    y_max = []\n",
    "    for child in root:\n",
    "        if child.tag == 'object':\n",
    "            for child2 in child:\n",
    "                if child2.tag == 'name':\n",
    "                    names.append(child2.text)\n",
    "                elif child2.tag == 'bndbox':\n",
    "                    for child3 in child2:\n",
    "                        if child3.tag == 'xmin':\n",
    "                            x_min.append(child3.text)\n",
    "                        elif child3.tag == 'xmax':\n",
    "                            x_max.append(child3.text)\n",
    "                        elif child3.tag == 'ymin':\n",
    "                            y_min.append(child3.text)\n",
    "                        elif child3.tag == 'ymax':\n",
    "                            y_max.append(child3.text)\n",
    "    category_and_bb = np.zeros([np.size(names), 5])\n",
    "    for i in range(np.size(names)):\n",
    "        category_and_bb[i][0] = get_id_of_class_name(names[i])\n",
    "        category_and_bb[i][1] = x_min[i]\n",
    "        category_and_bb[i][2] = x_max[i]\n",
    "        category_and_bb[i][3] = y_min[i]\n",
    "        category_and_bb[i][4] = y_max[i]\n",
    "    return category_and_bb\n",
    "\n",
    "\n",
    "def get_all_annotations(image_names, voc_path):\n",
    "    annotations = []\n",
    "    for i in range(np.size(image_names)):\n",
    "        image_name = image_names[0][i]\n",
    "        annotations.append(get_bb_of_gt_from_pascal_xml_annotation(image_name, voc_path))\n",
    "    return annotations\n",
    "\n",
    "\n",
    "def generate_bounding_box_from_annotation(annotation, image_shape):\n",
    "    length_annotation = annotation.shape[0]\n",
    "    masks = np.zeros([image_shape[0], image_shape[1], length_annotation])\n",
    "    for i in range(0, length_annotation):\n",
    "        masks[int(annotation[i, 3]):int(annotation[i, 4]), int(annotation[i, 1]):int(annotation[i, 2]), i] = 1\n",
    "    return masks\n",
    "\n",
    "\n",
    "def get_ids_objects_from_annotation(annotation):\n",
    "    return annotation[:, 0]\n",
    "\n",
    "\n",
    "def get_id_of_class_name (class_name):\n",
    "    if class_name == 'aeroplane':\n",
    "        return 1\n",
    "    elif class_name == 'bicycle':\n",
    "        return 2\n",
    "    elif class_name == 'bird':\n",
    "        return 3\n",
    "    elif class_name == 'boat':\n",
    "        return 4\n",
    "    elif class_name == 'bottle':\n",
    "        return 5\n",
    "    elif class_name == 'bus':\n",
    "        return 6\n",
    "    elif class_name == 'car':\n",
    "        return 7\n",
    "    elif class_name == 'cat':\n",
    "        return 8\n",
    "    elif class_name == 'chair':\n",
    "        return 9\n",
    "    elif class_name == 'cow':\n",
    "        return 10\n",
    "    elif class_name == 'diningtable':\n",
    "        return 11\n",
    "    elif class_name == 'dog':\n",
    "        return 12\n",
    "    elif class_name == 'horse':\n",
    "        return 13\n",
    "    elif class_name == 'motorbike':\n",
    "        return 14\n",
    "    elif class_name == 'person':\n",
    "        return 15\n",
    "    elif class_name == 'pottedplant':\n",
    "        return 16\n",
    "    elif class_name == 'sheep':\n",
    "        return 17\n",
    "    elif class_name == 'sofa':\n",
    "        return 18\n",
    "    elif class_name == 'train':\n",
    "        return 19\n",
    "    elif class_name == 'tvmonitor':\n",
    "        return 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e122d9",
   "metadata": {},
   "source": [
    "## Main Function to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f15e86f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of PASCAL VOC 2012 or other database to use for training\n",
    "path_voc = \"../data/voc/VOC2007/\"\n",
    "# path of other PASCAL VOC dataset, if you want to train with 2007 and 2012 train datasets\n",
    "path_voc2 = \"../data/voc/VOC2012/\"\n",
    "# path of where to store the models\n",
    "path_model = \"../model/models_image_zooms\"\n",
    "# path of where to store visualizations of search sequences\n",
    "path_testing_folder = '../results/voc/train/'\n",
    "# path of VGG16 weights\n",
    "path_vgg = \"../model/vgg16_weights.h5\"\n",
    "\n",
    "######## PARAMETERS ########\n",
    "epochs_id = 0\n",
    "# Class category of PASCAL that the RL agent will be searching\n",
    "class_object = 1\n",
    "# Scale of subregion for the hierarchical regions (to deal with 2/4, 3/4)\n",
    "scale_subregion = float(3)/4\n",
    "scale_mask = float(1)/(scale_subregion*4)\n",
    "# 1 if you want to obtain visualizations of the search for objects\n",
    "bool_draw = 0\n",
    "# How many steps can run the agent until finding one object\n",
    "number_of_steps = 10\n",
    "# Boolean to indicate if you want to use the two databases, or just one\n",
    "two_databases = 0\n",
    "epochs = 50\n",
    "gamma = 0.90\n",
    "epsilon = 1\n",
    "batch_size = 100\n",
    "# Pointer to where to store the last experience in the experience replay buffer,\n",
    "# actually there is a pointer for each PASCAL category, in case all categories\n",
    "# are trained at the same time\n",
    "h = np.zeros([20])\n",
    "# Each replay memory (one for each possible category) has a capacity of 100 experiences\n",
    "buffer_experience_replay = 1000\n",
    "# Init replay memories\n",
    "replay = [[] for i in range(20)]\n",
    "reward = 0\n",
    "\n",
    "######## MODELS ########\n",
    "\n",
    "model_vgg = obtain_compiled_vgg_16(path_vgg)\n",
    "\n",
    "# If you want to train it from first epoch, first option is selected. Otherwise,\n",
    "# when making checkpointing, weights of last stored weights are loaded for a particular class object\n",
    "\n",
    "if epochs_id == 0:\n",
    "    models = get_array_of_q_networks_for_pascal(\"0\", class_object)\n",
    "else:\n",
    "    models = get_array_of_q_networks_for_pascal(path_model, class_object)\n",
    "\n",
    "######## LOAD IMAGE NAMES ########\n",
    "\n",
    "if two_databases == 1:\n",
    "    image_names1 = np.array([load_images_names_in_data_set('trainval', path_voc)])\n",
    "    image_names2 = np.array([load_images_names_in_data_set('trainval', path_voc2)])\n",
    "    image_names = np.concatenate([image_names1, image_names2])\n",
    "else:\n",
    "    image_names = np.array([load_images_names_in_data_set('trainval', path_voc)])\n",
    "\n",
    "######## LOAD IMAGES ########\n",
    "\n",
    "if two_databases == 1:\n",
    "    images1 = get_all_images(image_names1, path_voc)\n",
    "    images2 = get_all_images(image_names2, path_voc2)\n",
    "    images = np.concatenate([images1, images2])\n",
    "else:\n",
    "    images = get_all_images(image_names, path_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a590fa85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec1bfdd881140eba9afaa52d463c13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b1dbb04a8420>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    118\u001b[0m                         iou, new_iou, last_matrix, index = follow_iou(gt_masks, region_mask,\n\u001b[1;32m    119\u001b[0m                                                                       \u001b[0marray_classes_gt_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_object\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                                                                       last_matrix, available_objects)\n\u001b[0m\u001b[1;32m    121\u001b[0m                         \u001b[0mgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_reward_movement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miou\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_iou\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-4e5528c06c8e>\u001b[0m in \u001b[0;36mfollow_iou\u001b[0;34m(gt_masks, mask, array_classes_gt_objects, object_id, last_matrix, available_objects)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mavailable_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0miou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_iou\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-4e5528c06c8e>\u001b[0m in \u001b[0;36mcalculate_iou\u001b[0;34m(img_mask, gt_mask)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimg_or\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbitwise_or\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_and\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_or\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0miou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0miou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcount_nonzero\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/project/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mcount_nonzero\u001b[0;34m(a, axis, keepdims)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \"\"\"\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmultiarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(epochs_id, epochs_id + epochs):\n",
    "    for j in trange(np.size(image_names)):\n",
    "        masked = 0\n",
    "        not_finished = 1\n",
    "        image = np.array(images[j])\n",
    "        image_name = image_names[0][j]\n",
    "        annotation = get_bb_of_gt_from_pascal_xml_annotation(image_name, path_voc)\n",
    "        if two_databases == 1:\n",
    "            if j < np.size(image_names1):\n",
    "                annotation = get_bb_of_gt_from_pascal_xml_annotation(image_name, path_voc)\n",
    "            else:\n",
    "                annotation = get_bb_of_gt_from_pascal_xml_annotation(image_name, path_voc2)\n",
    "        gt_masks = generate_bounding_box_from_annotation(annotation, image.shape)\n",
    "        array_classes_gt_objects = get_ids_objects_from_annotation(annotation)\n",
    "        region_mask = np.ones([image.shape[0], image.shape[1]])\n",
    "        shape_gt_masks = np.shape(gt_masks)\n",
    "        available_objects = np.ones(np.size(array_classes_gt_objects))\n",
    "        # Iterate through all the objects in the ground truth of an image\n",
    "        for k in range(np.size(array_classes_gt_objects)):\n",
    "            # Init visualization\n",
    "            background = Image.new('RGBA', (10000, 2500), (255, 255, 255, 255))\n",
    "            draw = ImageDraw.Draw(background)\n",
    "            # We check whether the ground truth object is of the target class category\n",
    "            if array_classes_gt_objects[k] == class_object:\n",
    "                gt_mask = gt_masks[:, :, k]\n",
    "                step = 0\n",
    "                new_iou = 0\n",
    "                # this matrix stores the IoU of each object of the ground-truth, just in case\n",
    "                # the agent changes of observed object\n",
    "                last_matrix = np.zeros([np.size(array_classes_gt_objects)])\n",
    "                region_image = image\n",
    "                offset = (0, 0)\n",
    "                size_mask = (image.shape[0], image.shape[1])\n",
    "                original_shape = size_mask\n",
    "                old_region_mask = region_mask\n",
    "                region_mask = np.ones([image.shape[0], image.shape[1]])\n",
    "                # If the ground truth object is already masked by other already found masks, do not\n",
    "                # use it for training\n",
    "                if masked == 1:\n",
    "                    for p in range(gt_masks.shape[2]):\n",
    "                        overlap = calculate_overlapping(old_region_mask, gt_masks[:, :, p])\n",
    "                        if overlap > 0.60:\n",
    "                            available_objects[p] = 0\n",
    "                # We check if there are still obejcts to be found\n",
    "                if np.count_nonzero(available_objects) == 0:\n",
    "                    not_finished = 0\n",
    "                # follow_iou function calculates at each time step which is the groun truth object\n",
    "                # that overlaps more with the visual region, so that we can calculate the rewards appropiately\n",
    "                iou, new_iou, last_matrix, index = follow_iou(gt_masks, region_mask, array_classes_gt_objects,\n",
    "                                                              class_object, last_matrix, available_objects)\n",
    "                new_iou = iou\n",
    "                gt_mask = gt_masks[:, :, index]\n",
    "                # init of the history vector that indicates past actions (6 actions * 4 steps in the memory)\n",
    "                history_vector = np.zeros([24])\n",
    "                # computation of the initial state\n",
    "                state = get_state(region_image, history_vector, model_vgg)\n",
    "                # status indicates whether the agent is still alive and has not triggered the terminal action\n",
    "                status = 1\n",
    "                action = 0\n",
    "                reward = 0\n",
    "                if step > number_of_steps:\n",
    "                    background = draw_sequences(i, k, step, action, draw, region_image, background,\n",
    "                                                path_testing_folder, iou, reward, gt_mask, region_mask, image_name,\n",
    "                                                bool_draw)\n",
    "                    step += 1\n",
    "                while (status == 1) & (step < number_of_steps) & not_finished:\n",
    "                    category = int(array_classes_gt_objects[k]-1)\n",
    "                    model = models[0][category]\n",
    "                    qval = model.predict(state.T, batch_size=1)\n",
    "                    background = draw_sequences(i, k, step, action, draw, region_image, background,\n",
    "                                                path_testing_folder, iou, reward, gt_mask, region_mask, image_name,\n",
    "                                                bool_draw)\n",
    "                    step += 1\n",
    "                    # we force terminal action in case actual IoU is higher than 0.5, to train faster the agent\n",
    "                    if (i < 100) & (new_iou > 0.5):\n",
    "                        action = 6\n",
    "                    # epsilon-greedy policy\n",
    "                    elif random.random() < epsilon:\n",
    "                        action = np.random.randint(1, 7)\n",
    "                    else:\n",
    "                        action = (np.argmax(qval))+1\n",
    "                    # terminal action\n",
    "                    if action == 6:\n",
    "                        iou, new_iou, last_matrix, index = follow_iou(gt_masks, region_mask,\n",
    "                                                                      array_classes_gt_objects, class_object,\n",
    "                                                                      last_matrix, available_objects)\n",
    "                        gt_mask = gt_masks[:, :, index]\n",
    "                        reward = get_reward_trigger(new_iou)\n",
    "                        background = draw_sequences(i, k, step, action, draw, region_image, background,\n",
    "                                                    path_testing_folder, iou, reward, gt_mask, region_mask,\n",
    "                                                    image_name, bool_draw)\n",
    "                        step += 1\n",
    "                    # movement action, we perform the crop of the corresponding subregion\n",
    "                    else:\n",
    "                        region_mask = np.zeros(original_shape)\n",
    "                        size_mask = (size_mask[0] * scale_subregion, size_mask[1] * scale_subregion)\n",
    "                        if action == 1:\n",
    "                            offset_aux = (0, 0)\n",
    "                        elif action == 2:\n",
    "                            offset_aux = (0, size_mask[1] * scale_mask)\n",
    "                            offset = (offset[0], offset[1] + size_mask[1] * scale_mask)\n",
    "                        elif action == 3:\n",
    "                            offset_aux = (size_mask[0] * scale_mask, 0)\n",
    "                            offset = (offset[0] + size_mask[0] * scale_mask, offset[1])\n",
    "                        elif action == 4:\n",
    "                            offset_aux = (size_mask[0] * scale_mask, \n",
    "                                          size_mask[1] * scale_mask)\n",
    "                            offset = (offset[0] + size_mask[0] * scale_mask,\n",
    "                                      offset[1] + size_mask[1] * scale_mask)\n",
    "                        elif action == 5:\n",
    "                            offset_aux = (size_mask[0] * scale_mask / 2,\n",
    "                                          size_mask[0] * scale_mask / 2)\n",
    "                            offset = (offset[0] + size_mask[0] * scale_mask / 2,\n",
    "                                      offset[1] + size_mask[0] * scale_mask / 2)\n",
    "                        region_image = region_image[int(offset_aux[0]):int(offset_aux[0] + size_mask[0]),\n",
    "                                       int(offset_aux[1]):int(offset_aux[1] + size_mask[1])]\n",
    "                        region_mask[int(offset[0]):int(offset[0] + size_mask[0]), int(offset[1]):int(offset[1] + size_mask[1])] = 1\n",
    "                        iou, new_iou, last_matrix, index = follow_iou(gt_masks, region_mask,\n",
    "                                                                      array_classes_gt_objects, class_object,\n",
    "                                                                      last_matrix, available_objects)\n",
    "                        gt_mask = gt_masks[:, :, index]\n",
    "                        reward = get_reward_movement(iou, new_iou)\n",
    "                        iou = new_iou\n",
    "                    history_vector = update_history_vector(history_vector, action)\n",
    "                    new_state = get_state(region_image, history_vector, model_vgg)\n",
    "                    # Experience replay storage\n",
    "                    if len(replay[category]) < buffer_experience_replay:\n",
    "                        replay[category].append((state, action, reward, new_state))\n",
    "                    else:\n",
    "                        if h[category] < (buffer_experience_replay-1):\n",
    "                            h[category] += 1\n",
    "                        else:\n",
    "                            h[category] = 0\n",
    "                        h_aux = h[category]\n",
    "                        h_aux = int(h_aux)\n",
    "                        replay[category][h_aux] = (state, action, reward, new_state)\n",
    "                        minibatch = random.sample(replay[category], batch_size)\n",
    "                        X_train = []\n",
    "                        y_train = []\n",
    "                        # we pick from the replay memory a sampled minibatch and generate the training samples\n",
    "                        for memory in minibatch:\n",
    "                            old_state, action, reward, new_state = memory\n",
    "                            old_qval = model.predict(old_state.T, batch_size=1)\n",
    "                            newQ = model.predict(new_state.T, batch_size=1)\n",
    "                            maxQ = np.max(newQ)\n",
    "                            y = np.zeros([1, 6])\n",
    "                            y = old_qval\n",
    "                            y = y.T\n",
    "                            if action != 6: #non-terminal state\n",
    "                                update = (reward + (gamma * maxQ))\n",
    "                            else: #terminal state\n",
    "                                update = reward\n",
    "                            y[action-1] = update #target output\n",
    "                            X_train.append(old_state)\n",
    "                            y_train.append(y)\n",
    "                        X_train = np.array(X_train)\n",
    "                        y_train = np.array(y_train)\n",
    "                        X_train = X_train.astype(\"float32\")\n",
    "                        y_train = y_train.astype(\"float32\")\n",
    "                        X_train = X_train[:, :, 0]\n",
    "                        y_train = y_train[:, :, 0]\n",
    "                        hist = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1, verbose=0)\n",
    "                        models[0][category] = model\n",
    "                        state = new_state\n",
    "                    if action == 6:\n",
    "                        status = 0\n",
    "                        masked = 1\n",
    "                        # we mask object found with ground-truth so that agent learns faster\n",
    "                        image = mask_image_with_mean_background(gt_mask, image)\n",
    "                    else:\n",
    "                        masked = 0\n",
    "                available_objects[index] = 0\n",
    "    if epsilon > 0.1:\n",
    "        epsilon -= 0.1\n",
    "    for t in range (np.size(models)):\n",
    "        if t == (class_object-1):\n",
    "            string = path_model + '/model' + str(t) + '_epoch_' + str(i) + 'h5'\n",
    "            string2 = path_model + '/model' + str(t) + 'h5'\n",
    "            model = models[0][t]\n",
    "            model.save_weights(string, overwrite=True)\n",
    "            model.save_weights(string2, overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
